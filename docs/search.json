[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(tidymodels)\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\n\nCode\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\nCode\nlibrary(baguette)"
  },
  {
    "objectID": "lab6.html#load-necessary-libraries",
    "href": "lab6.html#load-necessary-libraries",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(tidymodels)\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\n\nCode\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\nCode\nlibrary(baguette)"
  },
  {
    "objectID": "lab6.html#download-data-and-documentation-pdf",
    "href": "lab6.html#download-data-and-documentation-pdf",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Download data and documentation PDF",
    "text": "Download data and documentation PDF\n\n\nCode\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')"
  },
  {
    "objectID": "lab6.html#getting-basin-characteristics",
    "href": "lab6.html#getting-basin-characteristics",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Getting basin characteristics",
    "text": "Getting basin characteristics\n\n\nCode\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "lab6.html#question-1",
    "href": "lab6.html#question-1",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Question 1",
    "text": "Question 1\n\nMake sure all data and the PDF are downloaded into you data directory\n\n\nFrom the documentation PDF, report what zero_q_freq represents\nBased on the PDF, zero_q_freq represents the frequency of days with zero streamflow, which is recorded as a percentage."
  },
  {
    "objectID": "lab6.html#question-2",
    "href": "lab6.html#question-2",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Question 2",
    "text": "Question 2\n\nMake 2 maps of the sites, coloring the points by the aridty and p_mean column. Add clear labels, titles, and a color scale that makes sense for each parameter. Ensure these render as a single image with your choice of facet_*, patchwork, or ggpubr.\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(ggpubr)\n\nmap_aridity &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"lightyellow\", high = \"darkred\") +\n  ggtitle(\"Aridity at Each Site\") +\n  theme_map() +\n  labs(color = \"Aridity\")\n\nmap_p_mean &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"lightblue\", high = \"darkblue\") +\n  ggtitle(\"Precipitation (p_mean) at Each Site\") +\n  theme_map() +\n  labs(color = \"p_mean\")\n\naridity_and_percipitation &lt;- ggarrange(map_aridity, map_p_mean, ncol = 2, nrow = 1)\n\naridity_and_percipitation\n\n\n\n\n\n\n\n\n\nCode\nggsave(\"images/aridity_and_percipitation.png\", width = 10, height = 5, dpi = 300)"
  },
  {
    "objectID": "lab6.html#model-building",
    "href": "lab6.html#model-building",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Model Building",
    "text": "Model Building\n\nSplitting the data\n\n\nCode\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\n\n\nPreprocessor: recipe\n\n\nCode\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) |&gt;\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) |&gt;\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())"
  },
  {
    "objectID": "lab6.html#linear-regression-and-random-forest-models",
    "href": "lab6.html#linear-regression-and-random-forest-models",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Linear regression and random forest models",
    "text": "Linear regression and random forest models\n\n\nCode\nlm_model &lt;- linear_reg() |&gt;\n  # define the engine\n  set_engine(\"lm\") |&gt;\n  # define the mode\n  set_mode(\"regression\")\n\nrf_model &lt;- rand_forest() |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"regression\")"
  },
  {
    "objectID": "lab6.html#workflow_set-approach",
    "href": "lab6.html#workflow_set-approach",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Workflow_set approach",
    "text": "Workflow_set approach\n\n\nCode\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nCode\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     1\n2 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     1\n3 recipe_rand_fore… Prepro… rmse    0.565  0.0253    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.769  0.0263    10 recipe       rand…     2"
  },
  {
    "objectID": "lab6.html#question-3",
    "href": "lab6.html#question-3",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Question 3",
    "text": "Question 3\n\nBuild a xgboost (engine) regression (mode) model using boost_tree\n\n\nCode\nbt_model &lt;- boost_tree() |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"regression\")\n\n\n\n\nBuild a neural network model using the nnet engine from the baguette package using the bag_mlp function\n\n\nCode\nbag_model &lt;- bag_mlp() |&gt;\n  set_engine(\"nnet\") |&gt;\n  set_mode(\"regression\")\n\n\n\n\nAdd this to the above workflow\n\n\nCode\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, bt_model, bag_model)) |&gt;\n  workflow_map('fit_resamples', resamples = camels_cv) \n\n\n\n\nEvaluate the model and compare it to the linear and random forest models\n\n\nCode\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nCode\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.547  0.0262    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.789  0.0228    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.562  0.0256    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.771  0.0264    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\n\n\n\nWhich of the 4 models would you move forward with?\nI can see that the bag_mlp model outranks the other functions, so I would move forward with it."
  },
  {
    "objectID": "lab6.html#question-4",
    "href": "lab6.html#question-4",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Question 4",
    "text": "Question 4\n\nBorrowing from the workflow presented above, build your own complete ML pipeline to predict mean streamflow using the CAMELS dataset. You can experiment with different predictors and preprocessing steps to see how they impact model performance. A successful model will have a R-squared value &gt; 0.9."
  },
  {
    "objectID": "lab6.html#a-data-spliting",
    "href": "lab6.html#a-data-spliting",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "4a: Data Spliting",
    "text": "4a: Data Spliting\n\n\nCode\n# Set a seed for reproducible\nset.seed(123)\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Create an initial split with 75% used for training and 25% for testing\nsplit_data &lt;- initial_split(camels, prop = 0.75)\n\n# Extract your training and testing sets\ntrain_data &lt;- training(split_data)\ntest_data &lt;- testing(split_data)\n\n# Build a 10-fold CV dataset\ncv_splits &lt;- vfold_cv(train_data, v = 10)"
  },
  {
    "objectID": "lab6.html#b-recipe",
    "href": "lab6.html#b-recipe",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "4b: Recipe",
    "text": "4b: Recipe\n\n\nCode\ncamels |&gt; \n  select(q_mean, low_prec_freq, gvf_max) |&gt; \n  drop_na() |&gt;\n  cor()\n\n\n                  q_mean low_prec_freq    gvf_max\nq_mean         1.0000000    -0.7145711  0.4149484\nlow_prec_freq -0.7145711     1.0000000 -0.6478606\ngvf_max        0.4149484    -0.6478606  1.0000000\n\n\n\n\nCode\nggplot(camels, aes(x = low_prec_freq, y = gvf_max)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) +\n  labs(title = \"Dry Period Duration vs Green Vegetation Fraction vs Runnoff\", \n       x = \"Dry Period Duration\", \n       y = \"Green Vegetation Fraction\",\n       color = \"Mean Flow\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nDefine a formula you want to use to predict logQmean\n\n\nCode\nformula &lt;- logQmean ~ low_prec_freq + gvf_max\n\n\n\n\nDescribe in words why you are choosing the formula you are\nI have chosen to use low_prec_freq and gvf_max as predictors for q_mean because they have shown to both have a strong correlation to mean flow, and there is an inverse correlation between dry period duration and green vegetation fraction.\n\n\nBuild a recipe that you feel handles the predictors chosen well\n\n\nCode\nrecipe &lt;- recipe(formula, data = train_data) |&gt;\n  step_log(all_predictors()) |&gt;\n  step_interact(terms = ~ low_prec_freq:gvf_max) |&gt; \n  step_naomit(all_predictors(), all_outcomes())"
  },
  {
    "objectID": "lab6.html#c-define-3-models",
    "href": "lab6.html#c-define-3-models",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "4c: Define 3 models",
    "text": "4c: Define 3 models\n\nDefine a random forest model using the rand_forest function. Set the engine to ranger and the mode to regression. Define two other models of your choice\n\n\nCode\nnew_rf_model &lt;- rand_forest() |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"regression\")\n\nnew_bt_model &lt;- boost_tree() |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"regression\")\n\nnew_bag_model &lt;- bag_mlp() |&gt;\n  set_engine(\"nnet\") |&gt;\n  set_mode(\"regression\")"
  },
  {
    "objectID": "lab6.html#d-workflow-set",
    "href": "lab6.html#d-workflow-set",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "4d: Workflow set ()",
    "text": "4d: Workflow set ()\n\nWith your preprocessing steps and models defined, you can now build a workflow_set object to fit and evaluate your models. This will allow you to compare the performance of different models on the same data\n\n\nCode\nworkflow_set &lt;- workflow_set(list(recipe), list(new_rf_model, new_bt_model, new_bag_model)) |&gt;\n  workflow_map('fit_resamples', resamples = cv_splits)"
  },
  {
    "objectID": "lab6.html#e-evaluation",
    "href": "lab6.html#e-evaluation",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "4e: Evaluation",
    "text": "4e: Evaluation\n\nUse autoplot and rank_results to compare the models\n\n\nCode\nautoplot(workflow_set)\n\n\n\n\n\n\n\n\n\nCode\nrank_results(workflow_set, rank_metric = \"rsq\", select_best = TRUE)\n\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.709  0.0401    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.646  0.0420    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    0.742  0.0385    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.618  0.0412    10 recipe       boos…     2\n5 recipe_bag_mlp    Prepro… rmse    0.809  0.125     10 recipe       bag_…     3\n6 recipe_bag_mlp    Prepro… rsq     0.609  0.0700    10 recipe       bag_…     3\n\n\n\n\nDescribe what model you think is best and why\nIt looks like the random forest model is the best because it has the highest R-squared value (closest to a value of 1)."
  },
  {
    "objectID": "lab6.html#f-extact-and-evaluate",
    "href": "lab6.html#f-extact-and-evaluate",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "4f: Extact and Evaluate",
    "text": "4f: Extact and Evaluate"
  }
]